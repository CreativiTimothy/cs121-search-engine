"""
This code was mainly generated by AI
and then manually edited to fit assignment specifications.
"""

import json
import os
from pathlib import Path

from config import (
    INDEX_ROOT,
    LEXICON_PATH,
    DOC_META_PATH,
    DUPLICATE_MAP_PATH,
    UNIGRAM_INDEX_PATH,
    BIGRAM_INDEX_PATH,
    TRIGRAM_INDEX_PATH,
)

import analytic_io

def load_json(path: Path):
    with path.open("r", encoding="utf-8") as f:
        return json.load(f)

# ------------------------------------------------------------
#  Compute size of a single file safely
# ------------------------------------------------------------
def file_size_kb(path: Path):
    if not path.exists():
        return 0.0
    return round(os.path.getsize(path) / 1024, 2)

# ------------------------------------------------------------
#  Compute total index directory size
# ------------------------------------------------------------
def compute_index_size_kb():
    total = 0
    for root, _, files in os.walk(INDEX_ROOT):
        for f in files:
            total += os.path.getsize(os.path.join(root, f))
    return round(total / 1024, 2)

# ------------------------------------------------------------
#  Main analytics computation
# ------------------------------------------------------------
def compute_analytic():
    lexicon = load_json(LEXICON_PATH)
    doc_meta = load_json(DOC_META_PATH)

    analytic = analytic_io.load_analytic()

    # ----------------------------
    # Document counts
    # ----------------------------
    num_doc = len(doc_meta)

    # ----------------------------
    # Token counts
    # ----------------------------
    num_token_unigram = sum(
        1 for term, meta in lexicon.items() if meta["type"] == "unigram"
    )
    num_token_bigram = sum(
        1 for term, meta in lexicon.items() if meta["type"] == "bigram"
    )
    num_token_trigram = sum(
        1 for term, meta in lexicon.items() if meta["type"] == "trigram"
    )
    num_token_total = (
        num_token_unigram
        + num_token_bigram
        + num_token_trigram
    )

    # ----------------------------
    # Duplicate counts
    # ----------------------------
    num_exact = 0
    num_near = 0

    for meta in doc_meta.values():
        if meta.get("duplicate_of") is not None:
            if meta.get("duplicate_type") == "exact":
                num_exact += 1
            elif meta.get("duplicate_type") == "near":
                num_near += 1

    num_total = num_exact + num_near

    # ----------------------------
    # File sizes
    # ----------------------------
    size_unigram = file_size_kb(UNIGRAM_INDEX_PATH)
    size_bigram = file_size_kb(BIGRAM_INDEX_PATH)
    size_trigram = file_size_kb(TRIGRAM_INDEX_PATH)
    size_doc_meta = file_size_kb(DOC_META_PATH)
    size_duplicate = file_size_kb(DUPLICATE_MAP_PATH)
    size_lexicon = file_size_kb(LEXICON_PATH)

    size_total = round(
        size_unigram
        + size_bigram
        + size_trigram
        + size_doc_meta
        + size_duplicate
        + size_lexicon,
        2
    )

    # ----------------------------
    # Save analytics
    # ----------------------------
    analytic["indexing"] = {
        # Indexed document count
        "num_document": num_doc,

        # Token counts
        "num_token_total": num_token_total,
        "num_token_unigram": num_token_unigram,
        "num_token_bigram": num_token_bigram,
        "num_token_trigram": num_token_trigram,

        # Duplicate counts
        "num_duplicate_total": num_total,
        "num_duplicate_exact": num_exact,
        "num_duplicate_near": num_near,

        # Index file sizes
        "index_size_kb_total_kb": size_total,
        "index_size_kb_unigram_index": size_unigram,
        "index_size_kb_bigram_index": size_bigram,
        "index_size_kb_size_trigram_index": size_trigram,
        "index_size_kb_doc_meta": size_doc_meta,
        "index_size_kb_duplicate": size_duplicate,
        "index_size_kb_lexicon": size_lexicon,
    }

    analytic_io.save_analytic(analytic)

    print("Index analytics written to analytic.json")

if __name__ == "__main__":
    compute_analytic()
